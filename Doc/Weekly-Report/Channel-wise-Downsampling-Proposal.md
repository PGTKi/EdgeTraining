# channel-wise downsampling proposal

杨晨宇下一步的研究工作

## 主要内容

延续 *Stochastic Downsampling for Cost-Adjustable Inference and Improved Regularization in Convolutional Networks*  的down sampling思想，将其拓展到channel的粒度。原本的工作是再某一层加入一个起到down sampling作用的mean pooling，以此来减少计算量。我接下来的研究尝试着将其扩展到channel的粒度。



## 研究方向介绍

原文中的下采样在训练和推理两个阶段都有，在训练的过程中，随机选取下采样点和下采样率, 除了是文中后面达到可调节的效果的必要的一个条件之外，还可以提高最后的精度，提高鲁棒性(但是文中没有提及这样会不会节省训练的计算量，**还需要有待实验研究**)。在推理过程中，原文称可以根据目前人有的计算资源来选择下采样发生的位置。

**我们在训练和推理两个阶段中都有拓展的空间：**

在**训练过程**中，我们可以试试如果引入**随机分组下采样**，会不会更能提高训练出的网络的鲁棒性。这里我们先来介绍一下随机分组下采样

假设一个正常网络，设一个输入在从前往后传递到第L层的大小是$N_l$。那么随机分组下采样的就是在每一层分为两组，一组大小是$N_l$ 一组是$1/2 N_l$ 这样的。对于一个filter_luv, u是输入的feature map， v是输出的feature map，只有v的尺寸大于u，才使用这个filter，否则相当于是drop out掉。

我们可以研究一下这种新的训练过程能对神经网络带来怎样的影响，能否提高鲁棒性，能否减少总体的计算量。

在**推理过程**中，基于一个训练过程做了随机分组下采样的神经网络，我们可以研究如何怎样确定一种分组方式（下采样哪些channel），能够达到精度又高，又节省计算的效果，所以这相当于是一种新的**网络结构搜索**。而同一层中有几组不同尺寸的feature map也算是一种新型的结构。

这种网络结构搜索，由于搜索空间比较大，理论上每一个channel都要决定一下该不该下采样，所以需要使用自动化的方法，用启发式的，或者找到一些评判标准的。

结合**训练和推理**过程，由于下采样和卷积核的“成长”可以相互影响，所以还可以探索在训练过程中就有计划地分组下采样地可能性。

之所以要下采样，因为卷积核卷完的信息有冗余，但是如果在训练过程中加入了过分多地下采样，会不会使得卷积核更倾向于提高信息地冗余性，以此来对抗下采样造成的信息地丢失？

如果我们开始在训练中引入DS， 那么会影响filter的性质，如果filter的性质被改变，会影响下一层的channel的性质，从而影响我们这一个channel能否做DS的判断，这是一个什么样的循环呢？

## 研究内容介绍

为了探索上述可能性，我们需要研究很多内容。

1. 关于feature map是否应该大小不同的本质问题研究，看看一些feature map不同channel的熵做个比较，等等
2. 关于 down sampling会对卷积核做出什么样的影响的研究
3. 随机分组下采样的效果如何
4. 设计寻找一种新的结构: feature map中的不同channel的大小不同









