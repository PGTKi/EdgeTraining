{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是一个 minimal nnvm example.\n",
    "整个 workflow 就是先在第二个 code block 定义出合适的 computation graph, 然后在第三个 code block 进行反向。\n",
    "\n",
    "由于反向节点的 `_conv2d_grad` 等节点没有 `FTVMCompute` 和 `FTVMSchedule` ，所以目前会 block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnvm\n",
    "import tvm\n",
    "from nnvm import symbol\n",
    "from nnvm import graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = symbol.Variable(\"x\")\n",
    "conv1 = symbol.conv2d(x, channels=20, kernel_size=[3, 3], name=\"conv1\", use_bias=True)\n",
    "out_sum = symbol.sum(conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "---\n",
      "{\n",
      "  \"nodes\": [\n",
      "    {\n",
      "      \"op\": \"null\", \n",
      "      \"name\": \"x\", \n",
      "      \"inputs\": []\n",
      "    }, \n",
      "    {\n",
      "      \"op\": \"null\", \n",
      "      \"name\": \"conv1_weight\", \n",
      "      \"attrs\": {\n",
      "        \"channels\": \"20\", \n",
      "        \"kernel_size\": \"[3, 3]\", \n",
      "        \"use_bias\": \"True\"\n",
      "      }, \n",
      "      \"inputs\": []\n",
      "    }, \n",
      "    {\n",
      "      \"op\": \"null\", \n",
      "      \"name\": \"conv1_bias\", \n",
      "      \"attrs\": {\n",
      "        \"channels\": \"20\", \n",
      "        \"kernel_size\": \"[3, 3]\", \n",
      "        \"use_bias\": \"True\"\n",
      "      }, \n",
      "      \"inputs\": []\n",
      "    }, \n",
      "    {\n",
      "      \"op\": \"conv2d\", \n",
      "      \"name\": \"conv1\", \n",
      "      \"attrs\": {\n",
      "        \"channels\": \"20\", \n",
      "        \"kernel_size\": \"[3, 3]\", \n",
      "        \"use_bias\": \"True\"\n",
      "      }, \n",
      "      \"inputs\": [[0, 0, 0], [1, 0, 0], [2, 0, 0]]\n",
      "    }, \n",
      "    {\n",
      "      \"op\": \"sum\", \n",
      "      \"name\": \"sum0\", \n",
      "      \"inputs\": [[3, 0, 0]]\n",
      "    }, \n",
      "    {\n",
      "      \"op\": \"ones_like\", \n",
      "      \"name\": \"ones_like1\", \n",
      "      \"inputs\": [[4, 0, 0]]\n",
      "    }, \n",
      "    {\n",
      "      \"op\": \"expand_like\", \n",
      "      \"name\": \"sum0_grad\", \n",
      "      \"attrs\": {\n",
      "        \"axis\": \"[]\", \n",
      "        \"exclude\": \"1\"\n",
      "      }, \n",
      "      \"inputs\": [[5, 0, 0], [3, 0, 0]]\n",
      "    }, \n",
      "    {\n",
      "      \"op\": \"_conv2d_grad\", \n",
      "      \"name\": \"conv1_grad\", \n",
      "      \"attrs\": {\n",
      "        \"channels\": \"20\", \n",
      "        \"kernel_size\": \"[3, 3]\", \n",
      "        \"use_bias\": \"True\"\n",
      "      }, \n",
      "      \"inputs\": [[6, 0, 0], [0, 0, 0], [1, 0, 0]]\n",
      "    }\n",
      "  ], \n",
      "  \"arg_nodes\": [0, 1, 2], \n",
      "  \"node_row_ptr\": [0, 1, 2, 3, 4, 5, 6, 7, 10], \n",
      "  \"heads\": [[7, 0, 0]]\n",
      "}\n",
      "---\n",
      "Graph(%x, %conv1_weight, %conv1_bias) {\n",
      "  %3 = conv2d(%x, %conv1_weight, %conv1_bias, use_bias='True', kernel_size='[3, 3]', channels='20')\n",
      "  %4 = sum(%3)\n",
      "  %5 = ones_like(%4)\n",
      "  %6 = expand_like(%5, %3, exclude='1', axis='[]')\n",
      "  %7 = _conv2d_grad(%6, %x, %conv1_weight, channels='20', kernel_size='[3, 3]', use_bias='True')\n",
      "  ret %7.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "ys = out_sum\n",
    "xs = x\n",
    "\n",
    "g = graph.create(out_sum)\n",
    "g._set_symbol_list_attr('grad_ys', ys)\n",
    "g._set_symbol_list_attr('grad_xs', xs)\n",
    "ny = len(ys.list_output_names())\n",
    "grad_ys = [symbol.ones_like(ys[i]) for i in range(ny)]\n",
    "g._set_symbol_list_attr('grad_ys_out_grad', grad_ys)\n",
    "sym = g.apply('Gradient').symbol\n",
    "\n",
    "nx = len(xs) if isinstance(xs, list) else len(xs.list_output_names())\n",
    "ret = [sym[i] for i in range(nx)]\n",
    "print(len(ret))\n",
    "print(\"---\")\n",
    "print(graph.create(ret[0]).json())\n",
    "print(\"---\")\n",
    "print(graph.create(ret[0]).ir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...100%, 0.77 MB, 646 KB/s, 1 seconds passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot find config for target=llvm -device=arm_cpu -model=unknown, workload=('conv2d', (1, 28, 28, 3, 'float32'), (20, 28, 3, 3, 'float32'), (1, 1), (0, 0), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.\n"
     ]
    },
    {
     "ename": "NNVMError",
     "evalue": "[14:42:03] /Users/xuxiaoqiao/git-repos/tvm/nnvm/include/nnvm/op.h:532: Check failed: idx < data_.size() && data_[idx].second: Attribute FTVMCompute has not been registered for Operator _conv2d_grad\nStack trace:\n  [bt] (0) 1   libnnvm_compiler.dylib              0x00000001274d5cb3 dmlc::LogMessageFatal::~LogMessageFatal() + 67\n  [bt] (1) 2   libnnvm_compiler.dylib              0x00000001274d39c5 dmlc::LogMessageFatal::~LogMessageFatal() + 21\n  [bt] (2) 3   libnnvm_compiler.dylib              0x000000012752935b nnvm::OpMap<std::__1::function<tvm::Array<tvm::Tensor, void> (nnvm::NodeAttrs const&, tvm::Array<tvm::Tensor, void> const&, tvm::Array<tvm::Tensor, void> const&)> >::operator[](nnvm::Op const*) const + 603\n  [bt] (3) 4   libnnvm_compiler.dylib              0x00000001275262a2 nnvm::compiler::CompileEngine::GetScheduleArgs(nnvm::Graph, tvm::Array<tvm::Tensor, void> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*, tvm::Array<tvm::Tensor, void>*) + 5570\n  [bt] (4) 5   libnnvm_compiler.dylib              0x0000000127523ce4 nnvm::compiler::CompileEngine::DoLower(nnvm::Graph, tvm::Array<tvm::Tensor, void> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) + 196\n  [bt] (5) 6   libnnvm_compiler.dylib              0x000000012751f40c nnvm::compiler::CompileEngine::Lower(nnvm::Graph, tvm::Array<tvm::Tensor, void> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) + 780\n  [bt] (6) 7   libnnvm_compiler.dylib              0x000000012751f058 nnvm::compiler::GraphLower(nnvm::Graph, tvm::Array<tvm::Tensor, void> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) + 120\n  [bt] (7) 8   libnnvm_compiler.dylib              0x0000000127596878 nnvm::compiler::GraphCompile(nnvm::Graph const&) + 4136\n  [bt] (8) 9   libnnvm_compiler.dylib              0x000000012751e474 decltype(std::__1::forward<nnvm::Graph (*&)(nnvm::Graph const&)>(fp)(std::__1::forward<nnvm::Graph>(fp0))) std::__1::__invoke<nnvm::Graph (*&)(nnvm::Graph const&), nnvm::Graph>(nnvm::Graph (*&&&)(nnvm::Graph const&), nnvm::Graph&&) + 68\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNNVMError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8aff39f52c45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mg2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdeploy_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnnvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marm_cpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/git-repos/tvm/nnvm/python/nnvm/compiler/build_module.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(graph, target, shape, dtype, params, target_host, layout)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GraphFuse\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GraphCompile\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0mlibmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_attr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_move_out_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"module\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;31m# Write variable initial values into params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git-repos/tvm/nnvm/python/nnvm/graph.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, passes)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mghandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGraphHandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mnpass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_uint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNNGraphApplyPasses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnpass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mghandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mghandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git-repos/tvm/nnvm/python/nnvm/_base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \"\"\"\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNNVMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNNGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mc_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNNVMError\u001b[0m: [14:42:03] /Users/xuxiaoqiao/git-repos/tvm/nnvm/include/nnvm/op.h:532: Check failed: idx < data_.size() && data_[idx].second: Attribute FTVMCompute has not been registered for Operator _conv2d_grad\nStack trace:\n  [bt] (0) 1   libnnvm_compiler.dylib              0x00000001274d5cb3 dmlc::LogMessageFatal::~LogMessageFatal() + 67\n  [bt] (1) 2   libnnvm_compiler.dylib              0x00000001274d39c5 dmlc::LogMessageFatal::~LogMessageFatal() + 21\n  [bt] (2) 3   libnnvm_compiler.dylib              0x000000012752935b nnvm::OpMap<std::__1::function<tvm::Array<tvm::Tensor, void> (nnvm::NodeAttrs const&, tvm::Array<tvm::Tensor, void> const&, tvm::Array<tvm::Tensor, void> const&)> >::operator[](nnvm::Op const*) const + 603\n  [bt] (3) 4   libnnvm_compiler.dylib              0x00000001275262a2 nnvm::compiler::CompileEngine::GetScheduleArgs(nnvm::Graph, tvm::Array<tvm::Tensor, void> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*, tvm::Array<tvm::Tensor, void>*) + 5570\n  [bt] (4) 5   libnnvm_compiler.dylib              0x0000000127523ce4 nnvm::compiler::CompileEngine::DoLower(nnvm::Graph, tvm::Array<tvm::Tensor, void> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) + 196\n  [bt] (5) 6   libnnvm_compiler.dylib              0x000000012751f40c nnvm::compiler::CompileEngine::Lower(nnvm::Graph, tvm::Array<tvm::Tensor, void> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) + 780\n  [bt] (6) 7   libnnvm_compiler.dylib              0x000000012751f058 nnvm::compiler::GraphLower(nnvm::Graph, tvm::Array<tvm::Tensor, void> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) + 120\n  [bt] (7) 8   libnnvm_compiler.dylib              0x0000000127596878 nnvm::compiler::GraphCompile(nnvm::Graph const&) + 4136\n  [bt] (8) 9   libnnvm_compiler.dylib              0x000000012751e474 decltype(std::__1::forward<nnvm::Graph (*&)(nnvm::Graph const&)>(fp)(std::__1::forward<nnvm::Graph>(fp0))) std::__1::__invoke<nnvm::Graph (*&)(nnvm::Graph const&), nnvm::Graph>(nnvm::Graph (*&&&)(nnvm::Graph const&), nnvm::Graph&&) + 68\n\n"
     ]
    }
   ],
   "source": [
    "g2 = graph.create(ret[0])\n",
    "deploy_graph, lib, params = nnvm.compiler.build(g2, target=tvm.target.arm_cpu(), shape={\"x\": (1,28,28,3)}, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lib.imported_modules[0].get_source())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
