这篇文档是精简版的论文索引，用于一目了然地获知论文的核心思想，便于日后回顾以及共享。同时，希望精简的文档可以提高阅读的速度和更新时的效率。

| 标题                                                         | 关键词                                                       | 解决问题                                                     | 核心思想                                                     | 实现方法                                                     | 结果概述                                                     | 其他                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| AMC: AutoML for Model Compression and Acceleration on Mobile Devices （ECCV 2018） | structured pruning (channel)<br/>RL<br/>AutoML Model Compression | 使用**RL**自动化地寻找每一层的**压缩率**（剪枝目标的选择仍然是传统方法） | 从头到尾一层一层算作增强学习的一步，输入当前层信息输出剪枝率，整个网络剪完（不微调）直接测试精度 | RL state： 每一层基本信息加前一层action相关信息<br />RL action: 这一层的剪枝率<br />RL reward： 和目标有关, 结合 loss和FLOPS | CIFAR-10 computationally efficient: the RL can finish searching within 1 hour on a single GeForce GTX TITAN Xp GPU | 文中的普适性，仅仅是detection任务上的拓展，没有说在这个RL学到了适用于不同数据集和网络的统一策略 |
| Balanced Sparsity for Efficient DNN Inference on GPU         | Balanced Sparsity <br />GPU accelerator<br />                | 达到像fine-grained pruning 一样的压缩率，同时还能被GPU计算   | 把矩阵分成小块，在每一个小块里面的零的数目相同。这样GPU计算时就可以统一调度 |                                                              |                                                              |                                                              |
| OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER<br />ICLR 2017 | NN Conditional Computation<br />Mixture-of-Experts layer<br />LSTM<br />集群计算 | 设计一种**超大**的**条件分支**网络，在集群上解决由于分支导致batch size变小的问题 | 某一个MOE层，上千个expert网络，激活其中四个相加其结果。      | 在集群上，通过整合不同机器对某一个expert的激活，扩大expert的batch_size。 | 极大地增大网络的capacity，提高LSTM相关任务的精度             |                                                              |
|                                                              |                                                              |                                                              |                                                              |                                                              |                                                              |                                                              |

