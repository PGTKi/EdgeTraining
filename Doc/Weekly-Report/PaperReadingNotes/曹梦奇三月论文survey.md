| 题目                                                                                                      | 关键词                                           | 解决问题             | 核心思想                                                                                                                                                      | 实现方法 xxxxxxxx                                                         | 结果概述                                                                                                                                                    | 其它xxxxxxxxx                        |
| ------------------------------------------------------------------------------------------------------- | --------------------------------------------- | ---------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------- |
| Interpret Neural Networks by Identifying Critical Data Routing Paths (CVPR 2018)                        | Channel-wise Control Gate                     | 定量分析神经网络的可解释性    | 在卷积层通道后加入一组可训练的标量 (取名为 channel-wise control gates)，从而每个输入都可以通过这组标量进行表示。                                                                                   | 微调预训练模型，在每个卷积层后面添加一个随机初始化的标量，损失函数为原网络输出(不考虑label)与新网络输出的交叉熵，梯度更新这组标量。 | 在可解释性上，有效地建立了一种对抗样本检测的机制。在剪枝方向，经实验发现可以在不影响网络容量的基础上从CIFAR-100预训练VGG网络中蒸馏一个K分类的小网络。                                                                       |                                    |
| Dynamic Channel Pruning: Feature Boosting and Suppression (ICLR 2019)                                   | dynamic network, faster CNNs, channel pruning | memory-efficient | 受到boosting思想的启发，在卷积层后设计一种channel saliency predictor，通过前一层的特征去预测output channels的显著性。                                                                       | 原作者已开源在[github](https://github.com/deep-fry/mayo)                     | 该方法在VGG-16 和ResNet-18达到SOTA, 与之前的剪枝方法在加速比时保持更高的准确率。                                                                                                     | 值得follow的工作之一，考虑一种可预测通道显著性的CNN训练方法 |
| Rethinking the Value of Network Pruning (ICLR 2019)                                                     | 网络剪枝是一种NAS问题                                  |                  | 首先，对于具备预定义目标网络架构的剪枝算法，从随机初始化直接训练小目标模型能实现与使用经典三步流程相同的性能。在这种情况下，我们不需要从训练大规模模型开始，而是可以直接从头训练剪枝后模型。其次，对于没有预定义目标网络的剪枝算法，从头开始训练剪枝后的模型也可以实现与微调相当甚至更好的性能。（by 机器之心） | 比较两种网络的性能：“保留预训练模型参数的剪枝后模型” 与 “从头训练剪枝后的模型”                            | 我们需要重新审视现有的网络剪枝算法。第一阶段的过参数化似乎不像之前想象得那么有效。此外，继承大模型的权重不一定是最优选择，而且可能导致剪枝后的模型陷入局部极小值，即使这些权重按剪枝标准来看是「重要的」。该论文的结果显示自动剪枝算法的价值可能在于识别高效结构、执行隐性架构搜索，而不是选择「重要的」权重。 | 这篇论文质疑了神经网络剪枝工作的目的，但似乎有没有形成有用的结论。  |
| Channel Pruning for Accelerating Very Deep Neural Networks (ICCV 2017)                                  |                                               |                  |                                                                                                                                                           |                                                                       |                                                                                                                                                         |                                    |
| Runtime Neural Pruning (NIPS 2017)                                                                      | 通过强化学习进行网络剪枝                                  |                  |                                                                                                                                                           |                                                                       |                                                                                                                                                         |                                    |
| Learning Sparse Neural Networks through L0 Regularization (ICLR 2018)                                   |                                               |                  |                                                                                                                                                           |                                                                       |                                                                                                                                                         |                                    |
| AXNet: ApproXimate computing using an end-to-end trainable neural network                               | 近似计算                                          |                  |                                                                                                                                                           |                                                                       |                                                                                                                                                         |                                    |
| A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers (ECCV 2018) | ADMM                                          |                  |                                                                                                                                                           |                                                                       |                                                                                                                                                         |                                    |
