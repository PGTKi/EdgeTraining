| 题目                                                                                                      | 关键词                       | 解决问题          | 核心思想                                                                    | 实现方法                                                                  | 结果概述                                                                              | 其它  |
| ------------------------------------------------------------------------------------------------------- | ------------------------- | ------------- | ----------------------------------------------------------------------- | --------------------------------------------------------------------- | --------------------------------------------------------------------------------- | --- |
| Interpret Neural Networks by Identifying Critical Data Routing Paths (CVPR 2018)                        | Channel-wise Control Gate | 定量分析神经网络的可解释性 | 在卷积层通道后加入一组可训练的标量 (取名为 channel-wise control gates)，从而每个输入都可以通过这组标量进行表示。 | 微调预训练模型，在每个卷积层后面添加一个随机初始化的标量，损失函数为原网络输出(不考虑label)与新网络输出的交叉熵，梯度更新这组标量。 | 在可解释性上，有效地建立了一种对抗样本检测的机制。在剪枝方向，经实验发现可以在不影响网络容量的基础上从CIFAR-100预训练VGG网络中蒸馏一个K分类的小网络。 |     |
| Dynamic Channel Pruning: Feature Boosting and Suppression (ICLR 2019)                                   | 预训练网络剪枝                   |               |                                                                         |                                                                       |                                                                                   |     |
| Rethinking the Value of Network Pruning (ICLR 2019)                                                     | 网络剪枝是一种NAS问题              |               |                                                                         |                                                                       |                                                                                   |     |
| Channel Pruning for Accelerating Very Deep Neural Networks (ICCV 2017)                                  |                           |               |                                                                         |                                                                       |                                                                                   |     |
| Runtime Neural Pruning (NIPS 2017)                                                                      | 通过强化学习进行网络剪枝              |               |                                                                         |                                                                       |                                                                                   |     |
| Learning Sparse Neural Networks through L0 Regularization (ICLR 2018)                                   |                           |               |                                                                         |                                                                       |                                                                                   |     |
| AXNet: ApproXimate computing using an end-to-end trainable neural network                               | 近似计算                      |               |                                                                         |                                                                       |                                                                                   |     |
| A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers (ECCV 2018) | ADMM                      |               |                                                                         |                                                                       |                                                                                   |     |
