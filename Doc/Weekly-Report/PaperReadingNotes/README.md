# Paper Summary

此文档作阅读过的paper的总结性表格，请各位把相关的论文总结写一条在此表格中方便互享

- 若你觉得有可以添加的列或者比较新颖的点，请自行添加一列，写上对应内容。

- 若你觉得还需要更深入的讲解，可以单独写一个文档，在表格中填入文档连接，同时上传到仓库。

- 传入单独文档时，请把照片路径设为相对文件路径，并把照片放入”./Images/论文名“文件夹中方便显示

|                   标题xxxxxxxxxxxxxxxxxxx                    | 会议xxxxx  |    摘要翻译xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx     |              关键词xxxxxxxxxxxx               |         目标/   解决问题xxxxxxxx         |         核心思想/算法/实现方式xxxxxxxxxxxxxxxxxxxxxx         |                控制机制xxxxxxxxxxxxxxxxxxxxxx                |                  效果xxxxxxxxxxxxxxxxxxxxxx                  |                  其他xxxxxxxxxxxxxxxxxxxxxx                  |                         详细文档链接                         |
| :----------------------------------------------------------: | :--------: | :----------------------------------------------------------: | :-------------------------------------------: | :--------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| AXNet: ApproXimate computing using an end-to-end trainable neural network | ICCAD 2018 | 基于神经网路的近似计算是一个对容错性很高的应用节省大量计算的通用的架构。为保证近似的精度，现有的工作采用了两个神经网络的架构，一个为近似器，一个为预测器。近似器用于近似j计算结果，预测器预测给定数据是否能够安全地被预测（在给定精度的情况下）。但是将两个网络结合起来是不简单而且费时的，因为他们有不同的目标函数，他们需要被不同地训练。本文提出了一种新的网络架构AXNet将以上两种网络融合成一个整体的网络。在受到多任务学习（Multi-task Learning）的启发后设计的AXNet网络模型大大提高了激活率，而且减少了近似的误差。用于训练的资源也大大减小了。实验结果表明于前人的工作进行对比，此网络结构下有50.7%的激活率和训练时间被减少了 |                                               |                 近似计算                 | 设计一个可端到端训练的approximator和predictor结合的网络。具体方法是在predictor和approximator的每一层之间加入一个标量的对应元素的乘法算子，这样后向传播梯度的时候可以响应调整predictor的参数 | 在approximator的每一层和predictor的输出进行对于元素相乘。是用scalar product来控制。 | 预测准确率提高了，而且对于预测样本的近似误差更小（详见链接） |                                                              | <https://github.com/acada-sjtu/EdgeTraining/blob/master/Doc/Weekly-Report/PaperReadingNotes/AXNet%20ApproXimate%20computing%20using%20an%20end-to-end%20trainable%20neural%20network.md> |
|       RePr: Improved Training of Convolutional Filters       | CVPR 2019  | 既然网络中有些参数权重是多余，那我们训练的时候把他们丢弃（pruning），接着训练剩下的网络，为了不损失模型的capacity，然后再把丢弃的参数拿回来，效果是不是会好一点呢？基于这个想法，文章作者任务有几个重要的点：一是pruning哪些权重，而是如何再把丢弃的权重拿回来让他们发挥更大的作用。本文的一个贡献在于提出了一个metric，用于选择哪些filters丢弃。同时作者指出，即使是一个参数很少（ under-parameterized ）的网络，也会出现学到冗余的参数的情况，这不仅仅在多参数的网络中存在，原因就在于训练过程低效。 |                                               |  通过训练正交的特征，增强网络的泛化能力  | 不断的使用Prune的方法，临时的剔除掉一部分filter。之后再重新初始化，加入训练。 在Prune和重训练的过程中，尽量的让同一层的filter之间正交，这样就让filter之间没有太多重复的特征，能够增加更多有效的网络容量。 |                                                              |                                                              |                                                              |                                                              |
| PROXYLESSNAS: DIRECT NEURAL ARCHITECTURE SEARCH ON TARGET TASK AND HARDWARE | ICLR 2019  | 最原始的NAS方法需要消耗的计算量太大<br />所以需要proxy tasks，用小规模的数据，网络，或者训练数量来预计真实训练环境的结果<br />本文的方法是 proxylessNAS，直接学出一个适用大规模任务的架构，并且还是针对特定硬件的。<br />本文还解决了内存占用过高的问题，减少了计算代价<br />在CIFAR-10上，搜到的模型只有5.7M 参数，达到2.08% 错误率<br />在 ImageNet上，比MobileNetV2 高出3.1% top-1 accuracy，GPU 延时比之快1.2倍 |                                               | 直接在目标任务上的NAS，并且减少NAS的代价 | 把NAS视作一个剪枝的过程，path-level pruning<br />在一个over parameter的网络中，前后两个点之间可能又很多path（彼此独立），使用architecture parameter 控制其开闭<br />在本文中，architecture parameter代表使用这个path的概率 | 需要gradient based 方法来learn architecture parameter，因为把structure variable变成了概率，所以可以像logit一样传递梯度<br />文中提到两种处理延时的gradient 的方法，一种是建模拟合，一种是用REINFORCE。<br />直接使用gradient based的问题在于需要N(#path） times GPU memory compared to training a compact model。 因为经过一个batch，我可以改所有的structure variable，要把整个冗余的大网络过一遍。<br />韩松解决这个问题的方法是仅pairwise地比较，pairwise地改变梯度 | 在CIFAR-10上，搜到的模型只有5.7M 参数，达到2.08% 错误率<br />在 ImageNet上，比MobileNetV2 高出3.1% top-1 accuracy，GPU 延时比之快1.2倍 | 文中发现使用latency建模的效果比使用REINFORCE的好，因为毕竟建模就增加了一个信息是每一个block的所用的时间基本上是固定的<br />文章发现，对于GPU而言，模型浅宽比较好，7x7卷积核也比较好。<br />大的MBConv（mobile inverted bottleneck convolution） 层在downsampling的时候能保存更多信息 |                                                              |
|      prediction Based Execution on Deep Neural Networks      | AISCA 2018 | 这是一篇在软硬件层面进行CNN计算加速的文章，针对CNN RELU 只保留正数的特点，设计算法和硬件支持省去提前终止一定小于零的计算。这个工作作用于**细粒度**的卷积窗计算层面，可以看做一种对卷积运算的加速方法。<br />和传统的CNN加速器相比，平均提速28%,节能16%<br />在使用predictive mode牺牲3%loss 的时候，可以提速3.59x 和 3.14x <br />和static pruning相比，在不掉精度的条件下加速63%，节能49% |                                               |                                          | 对于一个kernel，把所有的卷积核从大到小排序，依次累加卷积核权重和激活的乘积。 由于feature map只有正的，这样就可以终止计算，因为发现0时后面再累加的结果也仅仅是负的权重和正的激活值的乘积，会被RELU过滤掉。<br />为了进一步加速，文中预测计算的结果会不会小于零。判断的方法是设定一个阈值**th**，和用来检验的**n**个权重的和，组成$(Th,n)$ 参数对。在计算的过程中，累加前n个权重的乘积结果，如果比Th小，则认为最后的结果会被RELU过滤掉。 | Th 和n 的参数对，本文是关于通过类似选超参数的思想学出来的。再具体一些的思路是：先对于每一个kernel 确定不同的参数对会带来多大的计算量以及精度损失（通过在数据集上实测），“profiling”这些信息。然后再组合不同kernel的参数配置，在数据集实测，微调。最后得到能满足精度要求的，计算量最小的参数对 | 和传统的CNN加速器相比，平均提速28%,节能16%<br />在使用predictive mode牺牲3%loss 的时候，可以提速3.59x 和 3.14x <br />和static pruning相比，在不掉精度的条件下加速63%，节能49% | 可以详见同目录里面的SnaPEA: Predictive Early Activation for Reducing Computation in Deep Convolutional Neural Networks 论文阅读 |                                                              |
| NullHop: A Flexible Convolutional Neural Network Accelerator Based on Sparse Representations of Feature Maps |            | 这文章主要是描述一个**神经网络加速器**，利用神经网络的激活的**稀疏性**来减少**前向传播**运算。这个加速器专门适用于**CNN**，**RELU**作为激活函数，并且对网络激活值的稀疏性有一定的要求。<br />实验中作者在XilinxZynq 上面尝试了五种CNN架构，28nm process以及500MHZ<br />VGG19可以达到超过450GOp/s，98%的MAC利用率，6.3mm 3Top/s/W的能耗效率 |                                               |                                          | **稀疏的表示方法**：对一个激活的feature map，经过RELU，只关注其不为零的数据。用Sparsity Map (**SM**) 表示非零元素的位置，用 Non-Zero Value List (**NZVL**)来表示为零元素的数值。<br />**利用稀疏的计算方法**：对于非零的激活值，计算这个激活值会对feature map哪些位置产生影响，对应乘kernel的值，在相应位置累加。 上述过程在不同输出feature map之间并行 | **工作流程**<br />压缩好的feature map从external DRAM 发送进来<br />Input Data Processor 解码<br />CCM 对pixel在相应位置做乘加运算<br />CCM从累加器推到output buffer<br />PRE把需要求和的feature map加起来 | 作者主要比较的时efficiency，还有effective Power Efficiency，作者在这几个方便都超过其它同类工作水平 | 可以详见同目录里面的readingnote<br />作者在实验分析中提到， VGG这样的大网络在这个架构上面的利用率比较高<br />但是VGG的第一个卷积层，以及自己设计的GigalNet（一个专门别扭这个芯片的小网络还有Roshambo Net 和一个小的 Face Detector）都卡在了output bus的带宽上面，主要的瓶颈都是IO。 |                                                              |
| Interpret Neural Networks by Identifying Critical Data Routing Paths | CVPR 2018  |                                                              |           Channel-wise Control Gate           |        定量分析神经网络的可解释性        | 在卷积层通道后加入一组可训练的标量 (取名为 channel-wise control gates)，从而每个输入都可以通过这组标量进行表示。微调预训练模型，在每个卷积层后面添加一个随机初始化的标量，损失函数为原网络输出(不考虑label)与新网络输出的交叉熵，梯度更新这组标量。 |                  Channel-wise Control Gate                   | 在可解释性上，有效地建立了一种对抗样本检测的机制。在剪枝方向，经实验发现可以在不影响网络容量的基础上从CIFAR-100预训练VGG网络中蒸馏一个K分类的小网络。 |                                                              |                                                              |
|  Dynamic Channel Pruning: Feature Boosting and Suppression   | ICLR 2019  |                                                              | dynamic network, faster CNNs, channel pruning |           节省计算时的内存开销           | 受到boosting思想的启发，在卷积层后设计一种channel saliency predictor，通过前一层的特征去预测output channels的显著性。 |                                                              | 该方法在VGG-16 和ResNet-18达到SOTA, 与之前的剪枝方法在加速比时保持更高的准确率。 | 值得follow的工作之一，考虑一种可预测通道显著性的CNN训练方法  |                                                              |
|           Rethinking the Value of Network Pruning            | ICLR 2019  |                                                              |             网络剪枝是一种NAS问题             |             思考网络剪枝方法             | 首先，对于具备预定义目标网络架构的剪枝算法，从随机初始化直接训练小目标模型能实现与使用经典三步流程相同的性能。在这种情况下，我们不需要从训练大规模模型开始，而是可以直接从头训练剪枝后模型。其次，对于没有预定义目标网络的剪枝算法，从头开始训练剪枝后的模型也可以实现与微调相当甚至更好的性能。文中比较两种网络的性能：“保留预训练模型参数的剪枝后模型” 与 “从头训练剪枝后的模型” |                                                              | 我们需要重新审视现有的网络剪枝算法。第一阶段的过参数化似乎不像之前想象得那么有效。此外，继承大模型的权重不一定是最优选择，而且可能导致剪枝后的模型陷入局部极小值，即使这些权重按剪枝标准来看是「重要的」。该论文的结果显示自动剪枝算法的价值可能在于识别高效结构、执行隐性架构搜索，而不是选择「重要的」权重。 | 这篇论文质疑了神经网络剪枝工作的目的，但似乎有没有形成有用的结论。 |                                                              |
|  Channel Pruning for Accelerating Very Deep Neural Networks  | ICCV 2017  |                                                              |                                               |                                          |                                                              |                                                              |                                                              |                                                              |                                                              |
|                    Runtime Neural Pruning                    | NIPS 2017  |                                                              |              强化学习，网络剪枝               |         通过强化学习进行网络剪枝         |                                                              |                                                              |                                                              |                                                              |                                                              |
|  Learning Sparse Neural Networks through L0 Regularization   | ICLR 2018  |                                                              |                                               |                                          |                                                              |                                                              |                                                              |                                                              |                                                              |
| Progressive DNN Compression: A Key to Achieve Ultra-High Weight Pruning and Quantization Rates using ADMM |            | 权重剪枝和权重量化是两个重要的深度神经网络（DNN）模型压缩方法。过去的研究都是都是基于一些启发式的算法做压缩。最近一篇论文提出了一种系统化的基于ADMM的剪枝方法，达到了目前来看剪枝的最好效果。在此论文中，为了增加结果的可行性和更快的收敛速度，我们**扩充了上述的基于一步ADMM优化的模型剪枝方法**，并且**运用到了权重量化上**。我们**在此基础上还提出了多步、渐进式的剪枝与量化方法**，此方法有以下优点：由于ADMM算法的正则项，多步渐进的方法可以达到更好的剪枝与量化的结果 减少了每一步ADMM的搜索空间。大量实验表明：分别在LeNet-5、AlexNet、ResNet-50上分别达到了246x、36x、8x的模型剪枝率，而且准确率几乎没有降低；在AlexNet上达到61x的剪枝率，并且比前人方法准确率降低得更少；首个在ResNet and MobileNet上达到比较好的剪枝率；首个在LeNet-5 for MNIST和VGG-16 for CIFAR-10上达到了无损的二进制量化。 |                  ADMM，剪枝                   |     基于ADMM的剪枝方法对模型进行压缩     | **扩充了上述的基于一步ADMM优化的模型剪枝方法**，并且**运用到了权重量化上**。我们**在此基础上还提出了多步、渐进式的剪枝与量化方法**，此方法有以下优点：由于ADMM算法的正则项，多步渐进的方法可以达到更好的剪枝与量化的结果 减少了每一步ADMM的搜索空间 | 运用ADMM优化方法进行剪枝，这样的机制应该是自行定义剪枝的机制 | 分别在LeNet-5、AlexNet、ResNet-50上分别达到了246x、36x、8x的模型剪枝率，而且准确率几乎没有降低；在AlexNet上达到61x的剪枝率，并且比前人方法准确率降低得更少；首个在ResNet and MobileNet上达到比较好的剪枝率；首个在LeNet-5 for MNIST和VGG-16 for CIFAR-10上达到了无损的二进制量化。 |                                                              |                                                              |
|                                                              |            |                                                              |                                               |                                          |                                                              |                                                              |                                                              |                                                              |                                                              |
|                                                              |            |                                                              |                                               |                                          |                                                              |                                                              |                                                              |                                                              |                                                              |
